{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_Product_ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O68bKkFTH20s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WK-vB1BkM-kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = '/content/drive/My Drive/BANGKIT/Capstone/Dataset/wood_dataset'"
      ],
      "metadata": {
        "id": "bSTFFFC4H-L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = []\n",
        "tag = []\n",
        "full_path = []\n",
        "for path, subdirs, files in os.walk(dataset):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name)) \n",
        "        tag.append(path.split('/')[-1])        \n",
        "        file_name.append(name)"
      ],
      "metadata": {
        "id": "9DKnhGloP1QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\"path\":full_path,'file_name':file_name,\"tag\":tag})\n",
        "df.groupby(['tag']).size()"
      ],
      "metadata": {
        "id": "2dRXHkf2P-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "C3XLgl4VQFdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X= df['path']\n",
        "y= df['tag']\n",
        "\n",
        "\n",
        "#Membagi dataset awal menjadi data train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=300)\n",
        "\n",
        "# kemudian data test dibagi menjadi 2 sehingga menjadi data test dan data validation.\n",
        "X_test, X_val, y_test, y_val = train_test_split(\n",
        "    X_test, y_test, test_size=0.5, random_state=100)\n",
        "\n",
        "# menyatukan kedalam masing-masing dataframe\n",
        "\n",
        "df_tr = pd.DataFrame({'path':X_train,'tag':y_train,'set':'train'})\n",
        "\n",
        "df_te = pd.DataFrame({'path':X_test,'tag':y_test,'set':'test'})\n",
        "\n",
        "df_val = pd.DataFrame({'path':X_val,'tag':y_val,'set':'validation'})"
      ],
      "metadata": {
        "id": "2pGsTiKlQNX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train size', len(df_tr))\n",
        "print('val size', len(df_te))\n",
        "print('test size', len(df_val))"
      ],
      "metadata": {
        "id": "W4m74hZaR2BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat pembagian pada masing masing set \n",
        "df_all = df_tr.append([df_te,df_val]).reset_index(drop=1)\\\n",
        "\n",
        "print('===================================================== \\n')\n",
        "print(df_all.groupby(['set','tag']).size(),'\\n')\n",
        "\n",
        "print('===================================================== \\n')\n",
        "\n",
        "#Mengecek sample datanya\n",
        "df_all.sample(3)"
      ],
      "metadata": {
        "id": "97kUHioVSjIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from tqdm.notebook import tqdm as tq\n",
        "\n",
        "\n",
        "\n",
        "datasource_path = '/content/drive/My Drive/BANGKIT/Capstone/Dataset/wood_dataset'\n",
        "dataset_path = \"dataset_wood/\"\n",
        "\n",
        "\n",
        "\n",
        "for index, row in tq(df_all.iterrows()):\n",
        "    \n",
        "   \n",
        "    file_path = row['path']\n",
        "    if os.path.exists(file_path) == False:\n",
        "            file_path = os.path.join(datasource_path,row['tag'],row['image'].split('.')[0])            \n",
        "    \n",
        "  \n",
        "    if os.path.exists(os.path.join(dataset_path,row['set'],row['tag'])) == False:\n",
        "        os.makedirs(os.path.join(dataset_path,row['set'],row['tag']))\n",
        "    \n",
        "    \n",
        "    destination_file_name = file_path.split('/')[-1]\n",
        "    file_dest = os.path.join(dataset_path,row['set'],row['tag'],destination_file_name)\n",
        "\n",
        "    if os.path.exists(file_dest) == False:\n",
        "        shutil.copy2(file_path,file_dest)"
      ],
      "metadata": {
        "id": "6XexN6LwJZiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Input Parameters\n",
        "dim = (150, 150)\n",
        "# dim = (456, 456)\n",
        "channel = (3, )\n",
        "input_shape = dim + channel\n",
        "\n",
        "#batch size\n",
        "batch_size = 10\n",
        "\n",
        "#Epoch\n",
        "epoch = 15"
      ],
      "metadata": {
        "id": "jsA1_05hsc1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "QTSvu9_gsiA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                 shear_range=0.2,\n",
        "                                 zoom_range=0.2,\n",
        "                                 horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                  shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True)"
      ],
      "metadata": {
        "id": "WUuHFTgLYO10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory('content/dataset_wood/train/',\n",
        "                                                    target_size=dim,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory('content/dataset_wood/validation/',\n",
        "                                                target_size=dim,\n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('content/dataset_wood/test/',\n",
        "                                                  target_size=dim,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True)\n",
        "\n",
        "num_class = test_generator.num_classes\n",
        "labels = train_generator.class_indices.keys()"
      ],
      "metadata": {
        "id": "XdbIl8wHLNMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)"
      ],
      "metadata": {
        "id": "O0psEhKiYRrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --pre efficientnet"
      ],
      "metadata": {
        "id": "bKIiaJXxMzLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = EfficientNetB2(\n",
        "    input_shape=input_shape,\n",
        "    include_top=False,\n",
        "    weights='noisy-student',\n",
        "    classes=num_class,\n",
        ")"
      ],
      "metadata": {
        "id": "DJUVS7n9YyMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers,Sequential\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "hrfQfUV5C73Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding custom layers\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(1024, activation=\"relu\")(x)\n",
        "\n",
        "predictions = layers.Dense(num_class, activation=\"softmax\")(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "e4E-V7MlC8k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "SBiQ8AEeDCnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "print('Compiling Model.......')\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mYxT4L3GDG-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 10\n",
        "history = model.fit(x=train_data,\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=EPOCH,\n",
        "        validation_data=val_data,\n",
        "        validation_steps=len(val_generator),\n",
        "        shuffle=True,\n",
        "        verbose = 1)"
      ],
      "metadata": {
        "id": "jHRRr4QoDIY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history['loss']"
      ],
      "metadata": {
        "id": "0dL--LsLDP3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history['accuracy']"
      ],
      "metadata": {
        "id": "TJNXauQIDQ4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) \n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cy8eR8KnYzHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history_augmented.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history_augmented.pkl')\n",
        "\n",
        "download_history()"
      ],
      "metadata": {
        "id": "jE_j4TmdY3zJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}